{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Lightning\n",
    "一个基于pytorch的深度学习训练框架，对训练流程进行了封装，可以简化代码。\n",
    "核心组件：`LightningModule`、`Trainer`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightningModule\n",
    "`LightningModule`将Pytorch代码组织为6个部分：\n",
    "- 计算（`init`)\n",
    "- 训练（`training_step`)\n",
    "- 验证循环（`validation_step`）\n",
    "- 测试循环（`test_step`）\n",
    "- 预测循环（`predict_step`）\n",
    "- 优化器和学习率策略（`configure_optimizers`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(28 * 28, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.relu(self.l1(x.view(x.size(0), -1)))\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.02)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上的模型继承了`pl.LightningModule`，可以通过`Trainer`对象进行调用。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_loader = DataLoader(MNIST(os.getcwd(), download=True, transform=transforms.ToTensor()))\n",
    "trainer = pl.Trainer(max_epochs=1)\n",
    "model = LitModel()\n",
    "\n",
    "trainer.fit(model, train_dataloaders=train_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LightningModule`封装了很多有用的方法，其中需要掌握的核心方法有：\n",
    "|Name|Description|\n",
    "|----|-----------|\n",
    "|init|定义计算|\n",
    "|forward|仅用于前向推理|\n",
    "|training_step|完整的训练循环|\n",
    "|validation_step|完整的验证循环|\n",
    "|test_step|完整的测试循环|\n",
    "|predict_step|完整的预测循环|\n",
    "|configure_optimizers|定义优化器和学习率策略|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练\n",
    "通过覆写`training_step()`方法激活训练循环："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitClassifier(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        return loss # 需要返回loss，或包含key为loss的字典"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用`self.log()`方法记录训练指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(self, batch, batch_idx):\n",
    "    x, y = batch\n",
    "    y_hat = self.model(x)\n",
    "    loss = F.cross_entropy(y_hat, y)\n",
    "\n",
    "    # logs metrics for each training_step,\n",
    "    # and the average across the epoch, to the progress bar and logger\n",
    "    self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "    return loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "程序会将每个step的输出收集到列表中，并传入`training_epoch_end(self, training_step_outputs)`方法，方便进行epoch层级的计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(self, batch, batch_idx):\n",
    "    x, y = batch\n",
    "    y_hat = self.model(x)\n",
    "    loss = F.cross_entropy(y_hat, y)\n",
    "    preds = ...\n",
    "    return {\"loss\": loss, \"other_stuff\": preds}\n",
    "\n",
    "\n",
    "def training_epoch_end(self, training_step_outputs):\n",
    "    all_preds = torch.stack(training_step_outputs)\n",
    "    ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多设备运算可以通过覆写`training_step_end(self, batch_parts)`方法定义每个step后的运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fit(self):\n",
    "    if global_rank == 0:\n",
    "        # prepare data is called on GLOBAL_ZERO only\n",
    "        prepare_data()\n",
    "\n",
    "    configure_callbacks()\n",
    "\n",
    "    with parallel(devices):\n",
    "        # devices can be GPUs, TPUs, ...\n",
    "        train_on_device(model)\n",
    "\n",
    "\n",
    "def train_on_device(model):\n",
    "    # called PER DEVICE\n",
    "    setup(\"fit\")\n",
    "    configure_optimizers()\n",
    "    on_fit_start()\n",
    "\n",
    "    # the sanity check runs here\n",
    "\n",
    "    on_train_start()\n",
    "    for epoch in epochs:\n",
    "        fit_loop()\n",
    "    on_train_end()\n",
    "\n",
    "    on_fit_end()\n",
    "    teardown(\"fit\")\n",
    "\n",
    "\n",
    "def fit_loop():\n",
    "    on_train_epoch_start()\n",
    "\n",
    "    for batch in train_dataloader():\n",
    "        on_train_batch_start()\n",
    "\n",
    "        on_before_batch_transfer()\n",
    "        transfer_batch_to_device()\n",
    "        on_after_batch_transfer()\n",
    "\n",
    "        training_step()\n",
    "\n",
    "        on_before_zero_grad()\n",
    "        optimizer_zero_grad()\n",
    "\n",
    "        on_before_backward()\n",
    "        backward()\n",
    "        on_after_backward()\n",
    "\n",
    "        on_before_optimizer_step()\n",
    "        configure_gradient_clipping()\n",
    "        optimizer_step()\n",
    "\n",
    "        on_train_batch_end()\n",
    "\n",
    "        if should_check_val:\n",
    "            val_loop()\n",
    "    # end training epoch\n",
    "    training_epoch_end()\n",
    "\n",
    "    on_train_epoch_end()\n",
    "\n",
    "\n",
    "def val_loop():\n",
    "    on_validation_model_eval()  # calls `model.eval()`\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    on_validation_start()\n",
    "    on_validation_epoch_start()\n",
    "\n",
    "    val_outs = []\n",
    "    for batch_idx, batch in enumerate(val_dataloader()):\n",
    "        on_validation_batch_start(batch, batch_idx)\n",
    "\n",
    "        batch = on_before_batch_transfer(batch)\n",
    "        batch = transfer_batch_to_device(batch)\n",
    "        batch = on_after_batch_transfer(batch)\n",
    "\n",
    "        out = validation_step(batch, batch_idx)\n",
    "\n",
    "        on_validation_batch_end(batch, batch_idx)\n",
    "        val_outs.append(out)\n",
    "\n",
    "    validation_epoch_end(val_outs)\n",
    "\n",
    "    on_validation_epoch_end()\n",
    "    on_validation_end()\n",
    "\n",
    "    # set up for train\n",
    "    on_validation_model_train()  # calls `model.train()`\n",
    "    torch.set_grad_enabled(True)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n",
    "通过覆写`validation_step()`方法激活验证循环。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitModel(pl.LightningModule):\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        # 如果不需要处理validation_step的输出，可以不返回任何值"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果要处理返回值，类似于`training_step()`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_step(self, batch, batch_idx):\n",
    "    x, y = batch\n",
    "    y_hat = self.model(x)\n",
    "    loss = F.cross_entropy(y_hat, y)\n",
    "    pred = ...\n",
    "    return pred\n",
    "\n",
    "\n",
    "def validation_epoch_end(self, validation_step_outputs):\n",
    "    all_preds = torch.stack(validation_step_outputs)\n",
    "    ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "测试（test）类似于验证（validation），需要覆写`test_step()`方法。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "默认`predict_step()`会运行`forward()`方法。可以通过简单覆写`predict_step()`定义其行为。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitMCdropoutModel(pl.LightningModule):\n",
    "    def __init__(self, model, mc_iteration):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.mc_iteration = mc_iteration\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        # enable Monte Carlo Dropout\n",
    "        self.dropout.train()\n",
    "\n",
    "        # take average of `self.mc_iteration` iterations\n",
    "        pred = torch.vstack([self.dropout(self.model(x)).unsqueeze(0) for _ in range(self.mc_iteration)]).mean(dim=0)\n",
    "        return pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以使用两种方式调用`predict()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# call after training\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m trainer \u001b[39m=\u001b[39m Trainer()\n\u001b[0;32m      3\u001b[0m trainer\u001b[39m.\u001b[39mfit(model)\n\u001b[0;32m      5\u001b[0m \u001b[39m# automatically auto-loads the best weights from the previous run\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Trainer' is not defined"
     ]
    }
   ],
   "source": [
    "# call after training\n",
    "trainer = Trainer()\n",
    "trainer.fit(model)\n",
    "\n",
    "# automatically auto-loads the best weights from the previous run\n",
    "predictions = trainer.predict(dataloaders=predict_dataloader)\n",
    "\n",
    "# or call with pretrained model\n",
    "model = MyLightningModule.load_from_checkpoint(PATH)\n",
    "trainer = Trainer()\n",
    "predictions = trainer.predict(model, dataloaders=test_dataloader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果想要执行推理，可以为`LightningModule`添加`forward()`方法。\n",
    "需要注意的是，这种情况下需要手动调用`eval()`方法和`no_grad()`上下文管理器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(pl.LightningModule):\n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "\n",
    "model = Autoencoder()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    reconstruction = model(embedding)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在生产中，可能存在多模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "\n",
    "class ClassificationTask(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model # 传入的模型\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, acc = self._shared_eval_step(batch, batch_idx)\n",
    "        metrics = {\"val_acc\": acc, \"val_loss\": loss}\n",
    "        self.log_dict(metrics)\n",
    "        return metrics\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, acc = self._shared_eval_step(batch, batch_idx)\n",
    "        metrics = {\"test_acc\": acc, \"test_loss\": loss}\n",
    "        self.log_dict(metrics)\n",
    "        return metrics\n",
    "\n",
    "    def _shared_eval_step(self, batch, batch_idx): # 用于validation_step和test_step的评价步骤\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        acc = accuracy(y_hat, y)\n",
    "        return loss, acc\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        return y_hat\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.model.parameters(), lr=0.02)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 保存超参数\n",
    "在`LightningModule`的`__init__`方法中使用`save_hyperparameters()`，可以存储`self.hparams`当中的所有属性。这些参数同时会存储在模型的`checkpoint`当中，大大方便了训练之后模型的恢复。如果`loggers`支持，将会自动记录`self.hparams`中的内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitMNIST(LightningModule):\n",
    "    def __init__(self, layer_1_dim=128, learning_rate=1e-2):\n",
    "        super().__init__()\n",
    "        # call this to save (layer_1_dim=128, learning_rate=1e-4) to the checkpoint\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # equivalent\n",
    "        self.save_hyperparameters(\"layer_1_dim\", \"learning_rate\")\n",
    "\n",
    "        # Now possible to access layer_1_dim from hparams\n",
    "        self.hparams.layer_1_dim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通常默认情况下任何传给`__init__`构造方法的参数都被认为是`LightningModule`的超参数，但是有时候有些参数不需要存储（例如不能够序列化存储的对象），可以明确指出排除这些参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitMNIST(LightningModule):\n",
    "    def __init__(self, loss_fx, generator_network, layer_1_dim=128):\n",
    "        super().__init__()\n",
    "        self.layer_1_dim = layer_1_dim\n",
    "        self.loss_fx = loss_fx\n",
    "\n",
    "        # call this to save only (layer_1_dim=128) to the checkpoint\n",
    "        self.save_hyperparameters(\"layer_1_dim\")\n",
    "\n",
    "        # equivalent\n",
    "        self.save_hyperparameters(ignore=[\"loss_fx\", \"generator_network\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load_from_checkpoint\n",
    "使用`save_hyperparameters()`自动存储了超参数的`LightningModule`可以通过`load_from_checkpoint()`方便的恢复。如果有参数被排除，那么需要在恢复时单独提供。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load specify the other args\n",
    "model = LitMNIST.load_from_checkpoint(PATH, loss_fx=torch.nn.SomeOtherLoss, generator_network=MyGenerator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch112",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "010eda6e9a4605d963edba6f6aa9e52c5868921d0d201a2cb7acfc2f94cccee8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
